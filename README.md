# Lab Assignment Submission

This repository contains the solutions for the take-home assignment.

## Q1: Vision Transformer on CIFAR-10

### How to Run
1.  Open `q1.ipynb` in Google Colab with a GPU runtime.
2.  Connect to Google Drive when prompted by the first cell.
3.  Run all cells from top to bottom. The notebook will train the model, save the best version to Google Drive, and report the final accuracy.

### Best Model Configuration
-   **Epochs Trained:** 15
-   **Patch Size:** 4x4
-   **Model Depth:** 7
-   **Embedding Dimension:** 384
-   **Optimizer:** AdamW with CosineAnnealingLR

### Results
| Model | Test Accuracy |
| :--- | :---: |
| ViT (15 Epochs) | 70.68% |

---

## Q2: Text-Driven Image Segmentation with SAM

### Pipeline Description
This notebook (`q2.ipynb`) demonstrates text-prompted image segmentation. The pipeline is as follows:
1.  **Text-to-Box:** It uses a pre-trained **GroundingDINO** model to take a text prompt (e.g., "a dog") and an image, and it outputs bounding boxes for the object described.
2.  **Box-to-Mask:** These bounding boxes are then fed as prompts to the **Segment Anything Model (SAM)**.
3.  **Segmentation:** SAM uses these box prompts to generate a precise segmentation mask for the object.
4.  **Visualization:** The final mask is overlaid on the original image using the `supervision` library.

### Limitations
-   The final mask's quality is highly dependent on the bounding box generated by GroundingDINO. Vague prompts can lead to incorrect boxes.
-   The current implementation is designed for a single object prompt per image.
-   **Note on SAM 2:** The assignment mentioned SAM 2, but due to its limited public availability, this implementation uses the original, industry-standard Segment Anything Model (SAM).
