Lab Assignment Submission

This repository contains solutions for the given take-home lab assignment.

Q1: Vision Transformer on CIFAR-10
How to Run

Open q1.ipynb in Google Colab with a GPU runtime enabled.

Connect to Google Drive when prompted by the first cell.

Run all cells sequentially from top to bottom.

The notebook will:

Train the model

Save the best-performing version to Google Drive

Display the final accuracy

Best Model Configuration
Parameter	Value
Epochs Trained	15
Patch Size	4√ó4
Model Depth	7
Embedding Dimension	384
Optimizer	AdamW with CosineAnnealingLR
Results
Model	Test Accuracy
ViT (15 Epochs)	70.68%
Q2: Text-Driven Image Segmentation with SAM
Pipeline Description

The notebook q2.ipynb demonstrates text-prompted image segmentation using pre-trained models.

Workflow Steps

Text-to-Box:
Uses GroundingDINO to take a text prompt (e.g., ‚Äúa dog‚Äù) and an image, generating bounding boxes for the described object.

Box-to-Mask:
The bounding boxes are passed as prompts to the Segment Anything Model (SAM).

Segmentation:
SAM produces a precise segmentation mask for the detected object.

Visualization:
The final mask is overlaid on the original image using the Supervision library.

Limitations

Mask quality depends heavily on the bounding boxes generated by GroundingDINO.
‚Üí Vague prompts may lead to inaccurate results.

The current implementation supports only one object prompt per image.

Note on SAM 2

Although the assignment mentioned SAM 2, its limited public availability led to the use of the original and widely adopted Segment Anything Model (SAM) by Meta AI for this implementation.

Repository Structure
üìÇ Lab-Assignment/
‚îú‚îÄ‚îÄ q1.ipynb   # Vision Transformer on CIFAR-10
‚îú‚îÄ‚îÄ q2.ipynb   # Text-Driven Image Segmentation using SAM
‚îî‚îÄ‚îÄ README.md  # Project Documentation

Acknowledgements

Vision Transformer (ViT) ‚Äî Google Research

GroundingDINO ‚Äî IDEA Research

Segment Anything Model (SAM) ‚Äî Meta AI

Supervision Library ‚Äî Roboflow
