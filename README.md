Q1: Vision Transformer on CIFAR-10
How to Run

Open q1.ipynb in Google Colab with a GPU runtime enabled.

Connect to Google Drive when prompted by the first cell.

Run all cells sequentially from top to bottom.

The notebook will train the model, save the best-performing version to Google Drive, and display the final accuracy.

Best Model Configuration
Parameter	Value
Epochs Trained	15
Patch Size	4√ó4
Model Depth	7
Embedding Dimension	384
Optimizer	AdamW with CosineAnnealingLR
Results
Model	Test Accuracy
ViT (15 Epochs)	XX.XX%
Q2: Text-Driven Image Segmentation with SAM
Pipeline Description

The notebook q2.ipynb demonstrates text-prompted image segmentation using pre-trained models.
The process involves the following steps:

Text-to-Box:
Utilizes the GroundingDINO model to take a text prompt (e.g., ‚Äúa dog‚Äù) and an image, generating bounding boxes for the described object.

Box-to-Mask:
These bounding boxes are passed as prompts to the Segment Anything Model (SAM).

Segmentation:
SAM produces a precise segmentation mask for the detected object.

Visualization:
The final mask is overlaid on the original image using the supervision library.

Limitations

The final mask quality heavily depends on the bounding boxes generated by GroundingDINO ‚Äî vague prompts may yield inaccurate results.

The current implementation supports only one object prompt per image.

Note on SAM 2:
Although the assignment mentioned SAM 2, its limited public availability led to the use of the original and widely adopted Segment Anything Model (SAM) for this implementation.

Repository Structure
üìÇ Lab-Assignment/
‚îú‚îÄ‚îÄ q1.ipynb     # Vision Transformer on CIFAR-10
‚îú‚îÄ‚îÄ q2.ipynb     # Text-Driven Image Segmentation using SAM
‚îî‚îÄ‚îÄ README.md    # Project Documentation

Acknowledgements

Vision Transformer (ViT) ‚Äî Google Research

GroundingDINO ‚Äî IDEA Research

Segment Anything Model (SAM) ‚Äî Meta AI

Supervision Library ‚Äî Roboflow
