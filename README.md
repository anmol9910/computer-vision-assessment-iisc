🧠 Lab Assignment Submission

This repository contains solutions for the given take-home lab assignment.

⚙️ Q1: Vision Transformer on CIFAR-10
🔹 How to Run

Open q1.ipynb in Google Colab with a GPU runtime enabled.

Connect to Google Drive when prompted by the first cell.

Run all cells sequentially from top to bottom.

The notebook will:

Train the model

Save the best-performing version to Google Drive

Display the final accuracy

📊 Best Model Configuration
Parameter	Value
Epochs Trained	15
Patch Size	4×4
Model Depth	7
Embedding Dimension	384
Optimizer	AdamW with CosineAnnealingLR
🧪 Results
Model	Test Accuracy
ViT (15 Epochs)	70.68%
🖼️ Q2: Text-Driven Image Segmentation with SAM
🔹 Pipeline Description

The notebook q2.ipynb demonstrates text-prompted image segmentation using pre-trained models.

Workflow Overview

Text-to-Box:
Uses GroundingDINO to take a text prompt (e.g., “a dog”) and an image, generating bounding boxes for the described object.

Box-to-Mask:
The bounding boxes are passed as prompts to the Segment Anything Model (SAM).

Segmentation:
SAM produces a precise segmentation mask for the detected object.

Visualization:
The final mask is overlaid on the original image using the Supervision library.

⚠️ Limitations

The mask quality depends heavily on the bounding boxes generated by GroundingDINO — vague prompts may yield inaccurate results.

The current implementation supports only one object prompt per image.

🧩 Note on SAM 2

Although the assignment mentioned SAM 2, its limited public availability led to the use of the original and widely adopted Segment Anything Model (SAM) by Meta AI for this implementation.

📂 Repository Structure
Lab-Assignment/
├── q1.ipynb   # Vision Transformer on CIFAR-10
├── q2.ipynb   # Text-Driven Image Segmentation using SAM
└── README.md  # Project Documentation

🙌 Acknowledgements

Vision Transformer (ViT) — Google Research

GroundingDINO — IDEA Research

Segment Anything Model (SAM) — Meta AI

Supervision Library — Roboflow
